{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compute JSD after storing the vectors of predictions.\n",
    "\n",
    "It also includes YTI dataset, but JSD, because the background masking applied for evaluation, is not very meaningful in this specific setting. \n",
    "\n",
    "Notebook is useful for visualizing the final table, with highlighted the best JSD per activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from typing import List\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_segments(vector):\n",
    "    '''\n",
    "    Return the lengths and labels of segments.\n",
    "    '''\n",
    "    if len(vector) == 0:\n",
    "        return []\n",
    "\n",
    "    # Find the indices where the value changes\n",
    "    change_indices = np.where(np.diff(vector) != 0)[0] + 1\n",
    "    # Include the start and end of the vector\n",
    "    segment_indices = np.concatenate(([0], change_indices, [len(vector)]))\n",
    "    # Calculate segment lengths\n",
    "    segment_lengths = np.diff(segment_indices)\n",
    "    # Get the segment values\n",
    "    segment_values = vector[segment_indices[:-1]]\n",
    "\n",
    "    return list(zip(segment_values, segment_lengths))\n",
    "\n",
    "def compute_js_divergence(hist1, hist2):    \n",
    "    '''\n",
    "    Compute the Jensen-Shannon divergence between two histograms.\n",
    "    '''\n",
    "    js_div = jensenshannon(hist1, hist2, base=2)\n",
    "    return js_div\n",
    "\n",
    "def compute_bins(max_segment, min_segment, bin_width):\n",
    "    '''\n",
    "    Compute bins for histogram based on the segment lengths.\n",
    "    '''\n",
    "    # # New max is the threshold after which we consider all segments as one bin\n",
    "    # # Add this for visualization only:\n",
    "    # new_max = max_segment/3\n",
    "    new_max = max_segment\n",
    "    num_bins = int(np.ceil((new_max - min_segment) / bin_width))  # Calculate number of bins\n",
    "    bins = [min_segment + i * bin_width for i in range(num_bins)]  # Create bin edges\n",
    "    bins.extend([max_segment])\n",
    "\n",
    "    return bins\n",
    "\n",
    "def custom_binning(segments, min_segment, max_segment, bin_width, return_quant=False):\n",
    "    '''\n",
    "    Compute histogram counts for the segment lengths.\n",
    "    '''\n",
    "    bins = compute_bins(max_segment, min_segment, bin_width)    \n",
    "    # Compute histogram counts\n",
    "    counts, _ = np.histogram(segments, bins=bins)\n",
    "    \n",
    "    # Create bins based on counts\n",
    "    quantized_bins = []\n",
    "    for i in range(len(counts)):\n",
    "        bin_segments = segments[(segments >= bins[i]) & (segments < bins[i+1])]\n",
    "        quantized_bins.append(list(bin_segments))\n",
    "    \n",
    "    if return_quant:\n",
    "        return bins, counts, quantized_bins\n",
    "    return bins, counts\n",
    "\n",
    "def compute_hists(vector, bin_width, max_length=None, return_segment_length=False): \n",
    "    '''\n",
    "    Compute histogram counts for the segment lengths.\n",
    "    '''\n",
    "    segments = count_segments(vector)\n",
    "    segment_lengths = [length for _, length in segments]\n",
    "    segment_lengths = np.array(segment_lengths)\n",
    "    if max_length is None:\n",
    "        max_length = len(vector)\n",
    "    custom_bins, counts = custom_binning(segment_lengths, 0, max_length, bin_width=bin_width)    \n",
    "    counts = np.array(counts)\n",
    "    if return_segment_length:\n",
    "        return counts, segment_lengths\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of computing the JS divergence for CTE and TOT\n",
    "vectors_path = 'YOUR_PATH'\n",
    "\n",
    "def compute_distributions(method, dataset, bin_width, max_length=None, return_base_hist=False, activities:List=None):\n",
    "    '''\n",
    "    Compute the JS divergence for the CTE and TOT methods.\n",
    "    Args:\n",
    "    - method: str, method to compute the JS divergence.\n",
    "    - dataset: str, dataset to compute the JS divergence.\n",
    "    - bin_width: int, width of the bins for the histograms.\n",
    "    - max_length: dict, maximum length of the videos for each\n",
    "    activity. If None, the maximum length is computed from the\n",
    "    predictions.\n",
    "    - return_base_hist: bool, if True, the histograms of the\n",
    "    gt distribution is returned.\n",
    "    - activities: list, activities to compute the JS divergence. If\n",
    "    None, the activities are set according to the dataset.\n",
    "    Returns:\n",
    "    - res: dict, JS divergence for each activity.\n",
    "    - base_hist: dict, base histograms for each activity.\n",
    "    '''\n",
    "    if activities is None and dataset == 'BF':\n",
    "        activities = ['coffee', 'cereals', 'tea', 'milk', 'juice', \n",
    "                'sandwich', 'scrambledegg', 'friedegg', 'salat', 'pancake']\n",
    "    elif activities is None and dataset == 'IKEA':\n",
    "        activities = ['Kallax_Shelf_Drawer', 'Lack_Coffee_Table', 'Lack_Side_Table', 'Lack_TV_Bench']\n",
    "    elif activities is None and dataset == 'YTI':\n",
    "        activities = ['changing_tire', 'cpr', 'jump_car', 'repot', 'coffee']\n",
    "        \n",
    "    if max_length is None:\n",
    "        max_length = {}\n",
    "\n",
    "    # Initialize variables\n",
    "    means_js = []\n",
    "    base_hist = {}\n",
    "    all_predictions = []\n",
    "\n",
    "    for activity in activities:\n",
    "        if activity not in max_length.keys():\n",
    "            max_length[activity] = None\n",
    "        '''\n",
    "        The predictions should be saved in a directory pointed by `path`.\n",
    "        The predictions should be saved in a pickle file with the following structure:\n",
    "        {\n",
    "            'gt': ([np.array], None), # Ground truth\n",
    "            0: ([np.array], dict) # Predictions, dictionary for matching the predicted labels with gt\n",
    "        }\n",
    "        '''\n",
    "        path = f'{vectors_path}/{dataset}/{method}/{activity}/'\n",
    "        jss = []        \n",
    "        hists = []\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'rb') as f:\n",
    "                segm = pickle.load(f)\n",
    "            gt = segm['gt'][0]\n",
    "            # Exclude background\n",
    "            gt = gt[gt != -1]\n",
    "            pred = segm[0][0]\n",
    "            all_predictions.append(pred)\n",
    "            \n",
    "            if max_length[activity] is None and dataset == 'YTI':\n",
    "                max_length[activity] = len(pred)\n",
    "\n",
    "            \n",
    "            video_name = file.removeprefix(f'{method}_')\n",
    "            if activity not in base_hist.keys():\n",
    "                base_hist[activity] = {}\n",
    "            if video_name not in base_hist[activity]:\n",
    "                base, gt_seg_len = compute_hists(gt, bin_width, max_length[activity], return_segment_length=True)\n",
    "                base_hist[activity][video_name] = base\n",
    "            else:\n",
    "                base = base_hist[activity][video_name]\n",
    "            hist, seg_len = compute_hists(pred, bin_width, max_length[activity], return_segment_length=True)\n",
    "\n",
    "            js_div = compute_js_divergence(base, hist)\n",
    "            jss.append(js_div)\n",
    "            hists.append(hist)\n",
    "\n",
    "        means_js.append(np.mean(jss))\n",
    "\n",
    "    res = {\n",
    "        'JS': means_js,\n",
    "        'all_preds': np.hstack(all_predictions),\n",
    "    }\n",
    "    if return_base_hist:\n",
    "        return res, base_hist\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed max_length for each activity in Breakfast\n",
    "max_length_activities = {\n",
    "    'coffee': 1121,\n",
    "    'cereals': 1004,\n",
    "    'tea': 994,\n",
    "    'milk': 1812,\n",
    "    'juice': 1771,\n",
    "    'sandwich': 3130,\n",
    "    'scrambledegg': 5944,\n",
    "    'friedegg': 8138,\n",
    "    'salat': 5445,\n",
    "    'pancake': 9341\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6dd5a_row0_col3, #T_6dd5a_row1_col3, #T_6dd5a_row2_col3, #T_6dd5a_row3_col3, #T_6dd5a_row4_col3, #T_6dd5a_row5_col3, #T_6dd5a_row6_col3, #T_6dd5a_row7_col3, #T_6dd5a_row8_col1, #T_6dd5a_row9_col3, #T_6dd5a_row10_col3 {\n",
       "  background-color: yellow;\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6dd5a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6dd5a_level0_col0\" class=\"col_heading level0 col0\" >n_frames</th>\n",
       "      <th id=\"T_6dd5a_level0_col1\" class=\"col_heading level0 col1\" >cte</th>\n",
       "      <th id=\"T_6dd5a_level0_col2\" class=\"col_heading level0 col2\" >tot</th>\n",
       "      <th id=\"T_6dd5a_level0_col3\" class=\"col_heading level0 col3\" >tot_tcl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6dd5a_row0_col0\" class=\"data row0 col0\" >97958.000000</td>\n",
       "      <td id=\"T_6dd5a_row0_col1\" class=\"data row0 col1\" >0.834573</td>\n",
       "      <td id=\"T_6dd5a_row0_col2\" class=\"data row0 col2\" >0.824994</td>\n",
       "      <td id=\"T_6dd5a_row0_col3\" class=\"data row0 col3\" >0.788244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6dd5a_row1_col0\" class=\"data row1 col0\" >129551.000000</td>\n",
       "      <td id=\"T_6dd5a_row1_col1\" class=\"data row1 col1\" >0.870401</td>\n",
       "      <td id=\"T_6dd5a_row1_col2\" class=\"data row1 col2\" >0.848207</td>\n",
       "      <td id=\"T_6dd5a_row1_col3\" class=\"data row1 col3\" >0.845250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6dd5a_row2_col0\" class=\"data row2 col0\" >131782.000000</td>\n",
       "      <td id=\"T_6dd5a_row2_col1\" class=\"data row2 col1\" >0.869641</td>\n",
       "      <td id=\"T_6dd5a_row2_col2\" class=\"data row2 col2\" >0.882234</td>\n",
       "      <td id=\"T_6dd5a_row2_col3\" class=\"data row2 col3\" >0.806441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6dd5a_row3_col0\" class=\"data row3 col0\" >177387.000000</td>\n",
       "      <td id=\"T_6dd5a_row3_col1\" class=\"data row3 col1\" >0.880183</td>\n",
       "      <td id=\"T_6dd5a_row3_col2\" class=\"data row3 col2\" >0.862942</td>\n",
       "      <td id=\"T_6dd5a_row3_col3\" class=\"data row3 col3\" >0.861640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6dd5a_row4_col0\" class=\"data row4 col0\" >241462.000000</td>\n",
       "      <td id=\"T_6dd5a_row4_col1\" class=\"data row4 col1\" >0.864508</td>\n",
       "      <td id=\"T_6dd5a_row4_col2\" class=\"data row4 col2\" >0.908549</td>\n",
       "      <td id=\"T_6dd5a_row4_col3\" class=\"data row4 col3\" >0.834595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6dd5a_row5_col0\" class=\"data row5 col0\" >259495.000000</td>\n",
       "      <td id=\"T_6dd5a_row5_col1\" class=\"data row5 col1\" >0.908274</td>\n",
       "      <td id=\"T_6dd5a_row5_col2\" class=\"data row5 col2\" >0.928674</td>\n",
       "      <td id=\"T_6dd5a_row5_col3\" class=\"data row5 col3\" >0.844862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6dd5a_row6_col0\" class=\"data row6 col0\" >517478.000000</td>\n",
       "      <td id=\"T_6dd5a_row6_col1\" class=\"data row6 col1\" >0.873144</td>\n",
       "      <td id=\"T_6dd5a_row6_col2\" class=\"data row6 col2\" >0.896093</td>\n",
       "      <td id=\"T_6dd5a_row6_col3\" class=\"data row6 col3\" >0.838682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_6dd5a_row7_col0\" class=\"data row7 col0\" >539733.000000</td>\n",
       "      <td id=\"T_6dd5a_row7_col1\" class=\"data row7 col1\" >0.888757</td>\n",
       "      <td id=\"T_6dd5a_row7_col2\" class=\"data row7 col2\" >0.902804</td>\n",
       "      <td id=\"T_6dd5a_row7_col3\" class=\"data row7 col3\" >0.867361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_6dd5a_row8_col0\" class=\"data row8 col0\" >558928.000000</td>\n",
       "      <td id=\"T_6dd5a_row8_col1\" class=\"data row8 col1\" >0.902603</td>\n",
       "      <td id=\"T_6dd5a_row8_col2\" class=\"data row8 col2\" >0.905874</td>\n",
       "      <td id=\"T_6dd5a_row8_col3\" class=\"data row8 col3\" >0.903815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_6dd5a_row9_col0\" class=\"data row9 col0\" >937125.000000</td>\n",
       "      <td id=\"T_6dd5a_row9_col1\" class=\"data row9 col1\" >0.903957</td>\n",
       "      <td id=\"T_6dd5a_row9_col2\" class=\"data row9 col2\" >0.917168</td>\n",
       "      <td id=\"T_6dd5a_row9_col3\" class=\"data row9 col3\" >0.857930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dd5a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_6dd5a_row10_col0\" class=\"data row10 col0\" >nan</td>\n",
       "      <td id=\"T_6dd5a_row10_col1\" class=\"data row10 col1\" >0.889143</td>\n",
       "      <td id=\"T_6dd5a_row10_col2\" class=\"data row10 col2\" >0.901503</td>\n",
       "      <td id=\"T_6dd5a_row10_col3\" class=\"data row10 col3\" >0.857138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7b4efeabc090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'JS'\n",
    "dfs = {}\n",
    "dataset = \"BF\"\n",
    "bin_width = 20\n",
    "\n",
    "# `Base_hist` represents the histograms of the ground truth distribution\n",
    "base_hist = {}\n",
    "# `max_length_activities` represents the maximum length of the videos for each activity\n",
    "# max_length_activities = None\n",
    "# Compute distributions for CTE, TOT and TOT+TCL\n",
    "cte, base_hist = compute_distributions('cte', dataset, bin_width, return_base_hist=True, max_length=max_length_activities)\n",
    "tot = compute_distributions('tot', dataset, bin_width, max_length=max_length_activities)\n",
    "tot_tcl = compute_distributions('tot_tcl', dataset, bin_width, max_length=max_length_activities)\n",
    "\n",
    "# For each datasets, the number of total frame per activity is set\n",
    "if dataset == 'BF':\n",
    "    # 'coffee', 'cereals', 'tea', 'milk', 'juice', 'sandwich', 'scrambledegg', 'friedegg', 'salat', 'pancake'\n",
    "    total_frames = [97958, 129551, 131782, 177387, 241462, 259495, 517478, 539733, 558928, 937125]\n",
    "elif dataset == 'IKEA':\n",
    "    # 'Kallax_Shelf_Drawer', 'Lack_Coffee_Table', 'Lack_Side_Table', 'Lack_TV_Bench'\n",
    "    total_frames = [493635, 972309, 747453, 831693]\n",
    "elif dataset == 'YTI':\n",
    "    # 'changing tire', 'coffee', 'cpr', 'jump car', 'repot'\n",
    "    total_frames = [20158, 20705, 11220, 12546, 12345]\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'n_frames': total_frames,\n",
    "    'cte': cte[metric],\n",
    "    'tot': tot[metric],\n",
    "    'tot_tcl': tot_tcl[metric],\n",
    "})\n",
    "\n",
    "# Highlight minimum value per row with black text (JSD lower is better)\n",
    "def highlight_min(x):\n",
    "    is_min = x == x.min()\n",
    "    styles = np.where(is_min, 'background-color: yellow; color: black', '')\n",
    "    return styles\n",
    "\n",
    "# Calculate weighted average of all the columns based on column 1, the number of frames\n",
    "weighted_avg = np.average(df[['cte', 'tot', 'tot_tcl']], weights=df['n_frames'], axis=0)\n",
    "\n",
    "# Create a DataFrame for the weighted averages\n",
    "weighted_avg_df = pd.DataFrame([weighted_avg], columns=['cte', 'tot', 'tot_tcl'])\n",
    "\n",
    "# Append the weighted averages as the last row to the original DataFrame\n",
    "df = pd.concat([df, weighted_avg_df], ignore_index=True)\n",
    "\n",
    "# Apply the style using applymap\n",
    "styled_df = df.style.apply(lambda x: highlight_min(x), axis=1)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "dfs[bin_width] = styled_df\n",
    "display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
